---
layout: ../layouts/Layout.astro
title: HIVE
---

import { Tweet, Vimeo, YouTube } from "astro-embed";

# Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control

<div class="authors">
  <p>Timothée Anne, [Noah Syrkis](https://syrkis.com), Meriem Elhosni, [Sebastian Risi](https://sebastianrisi.com/)</p>
</div>

<iframe src="https://drive.google.com/file/d/1gIoexEmRMqccVzDLCN9TQ-u0VhJU33wU/preview" allow="autoplay"></iframe>

## Abstract

Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks. A promising but largely underexplored area is their potential to facilitate human coordination with large numbers of agents. Such capabilities would be very useful in domains including disaster response, urban planning, and real-time strategy scenarios. In this work, we introduce a real-time strategy game benchmark designed to evaluate these abilities, alongside a novel framework we term HIVE. HIVE empowers a single human to coordinate swarms of up to 1,500 agents using natural language dialog with an LLM. We present promising results on this multi-agent benchmark, with the hybrid approach excelling in tasks such as coordinating agent movements, exploiting unit weaknesses, leveraging human annotations, and understanding terrain and infrastructure. However, our findings also highlight critical limitations of current models, including difficulties in processing visual information and challenges in formulating long-term strategic plans. This work sheds light on the potential and limitations of LLMs in human-swarm coordination, paving the way for future research in this exciting area.

![](/method_overview.svg)

## Introduction

Artificial intelligence (AI) is increasingly being employed across a diverse range of domains, with recent explorations into its application in defense settings. As these systems acquire the capability to act in a growing number of areas—often without established quality guarantees—the risks associated with automation bias become more significant. Intuitively, technologies such as large language models (LLMs) and their multi-modal counterparts (MLLMs) can be positioned between observation and action in strategic war games in several ways, including:

_1) As end-to-end controllers._

_2) As subordinate to a human._

_3) Collaborating with a human._

Previous research has demonstrated that, to date, these systems do not perform adequately in the first role. Thus, human involvement remains essential in strategic domains, not only for ethical and legal reasons, but also performance reasons.

In this paper, we present a framework capable of evaluating all three modes of AI function. As AI technology advances, we expect to see performance improvements across all three modes.

## The HIVE Approach

## Results

## BibTeX

```
@article{anne2025hive,
  title = {Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control},
  author = {Timothée Anne and Noah Syrkis and Meriem Elhosni and Sebastian Risi},
  year = {2025},
}
```
